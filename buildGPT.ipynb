{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benteaux/karpathy-tutorials/blob/main/buildGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk1sNzKIq0Bu",
        "outputId": "bdef5c52-e93a-4592-f156-1c5521725d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-17 02:01:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-17 02:01:24 (26.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# tiny shakespeare\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ThrRBafqz3O"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g1G9ED8u7R_",
        "outputId": "023fa6ad-fb9f-4c21-e710-ce3e3ac2ed08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRYI0E68vnYu"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = max_iters // 10\n",
        "eval_iters = 100\n",
        "n_embd = 192\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "lr = 3e-3\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkNjJPcWv-1_"
      },
      "outputs": [],
      "source": [
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDVJ5hZrwaj-",
        "outputId": "5393f764-eddb-42a2-fcd4-914199aa2f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJB9nVgHxA_e",
        "outputId": "2213d907-389f-4cc1-cf1b-d54ca483467a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaBVtAg3xEB3"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFXMnqj6x_ko",
        "outputId": "81af1460-37f0-49e0-d5fc-b0306b9203a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets: \n",
            "torch.Size([4, 8])\n",
            " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(f'inputs:\\n{xb.shape}\\n{xb}\\ntargets: \\n{yb.shape}\\n {yb}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prr3Z7G5ct65"
      },
      "outputs": [],
      "source": [
        "# for occasionally evaluating the overall loss while training\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval() # set model to evaluation mode\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      x, y = get_batch(split)\n",
        "      logits, loss = model(x, y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train() # set model to training mode\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vlqGmEgKLUy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, n_heads):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, n_heads, bias = False)\n",
        "    self.query = nn.Linear(n_embd, n_heads, bias = False)\n",
        "    self.value = nn.Linear(n_embd, n_heads, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B, T, C)\n",
        "    q = self.key(x) # (B, T,C)\n",
        "    v = self.key(x) # (B, T, C)\n",
        "    product = q @ k.transpose(-2, -1) # (B, T, T)\n",
        "    scaled = product * C ** -0.5\n",
        "    # print(f'scaled shape: {scaled.shape}')\n",
        "    # print(f'x shape: {x.shape}')\n",
        "    wei = scaled.masked_fill(scaled[:, :T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "    wei = self.dropout(wei)\n",
        "    result = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLMlG6MYPAfW"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for h in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZCVBkwrwFH3"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, eps = 1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    xmean = x.mean(1, keepdim = True) # row mean\n",
        "    y = x + self.eps\n",
        "    xvar = y.var(1, keepdim = True) # row variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    print(f'xmean: {xmean}')\n",
        "    print(f'xvar: {xvar}')\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9oPmiK2rcZT"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 *n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az3XSWTWta29"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_heads\n",
        "    self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x)) # skip / residual connections\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp0O_sK3LAvt",
        "outputId": "3d4a4305-ce03-433d-edf1-a0443575a19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 65]) tensor(4.2651, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "    self.ln = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # idx & targets are (B, T) integer tensors\n",
        "    embeddings = self.token_embedding_table(idx) # (B, T, C)\n",
        "    B,T,C = embeddings.shape\n",
        "    position_embeddings = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
        "    x = embeddings + position_embeddings # (B, T, C)\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) # pytorch expects (B, T)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T)\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      logits = logits[:, -1, :] # (B, C), taking the last timestep and getting rid of the time dimension\n",
        "      probs = F.softmax(logits, dim = 1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQBJONfbN-4s",
        "outputId": "b9eeaa64-f085-4381-9d22-d93fcb4edb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fKRAK?Tzw\n"
          ]
        }
      ],
      "source": [
        "idx = torch.zeros((1, 1), dtype = torch.long) # 1 batch w/ idx 1, the newline character\n",
        "print(decode(model.generate(idx, max_new_tokens = 100)[0].tolist()))\n",
        "# indexing [0] b/c we want all the timesteps from the first (our only) batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANCrQP3sP1GU"
      },
      "outputs": [],
      "source": [
        "# pytrorch optimizer\n",
        "opt = torch.optim.Adam(model.parameters(), lr = lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShVWXULMQVe7",
        "outputId": "8bec06c6-178a-4252-e1f5-2f054168b33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.2518, val loss 4.2608\n",
            "step 500: train loss 0.3454, val loss 0.3481\n",
            "step 1000: train loss 0.3362, val loss 0.3407\n",
            "step 1500: train loss 0.3218, val loss 0.3255\n",
            "step 2000: train loss 0.3163, val loss 0.3196\n",
            "step 2500: train loss 0.3133, val loss 0.3126\n",
            "step 3000: train loss 0.3127, val loss 0.3126\n",
            "step 3500: train loss 0.3016, val loss 0.3037\n",
            "step 4000: train loss 0.3047, val loss 0.3063\n",
            "step 4500: train loss 0.3027, val loss 0.3030\n",
            "0.30612465739250183\n"
          ]
        }
      ],
      "source": [
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f'step {steps}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
        "\n",
        "  # sample batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  opt.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b8IUkn7NQpBw",
        "outputId": "42cb0dcd-9338-4c7d-bacc-568fe8bf2c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rean ilt \n"
          ]
        }
      ],
      "source": [
        "idx = torch.zeros((1, 1), dtype = torch.long) # 1 batch w/ idx 1, the newline character\n",
        "print(decode(model.generate(idx, max_new_tokens = 500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KocqX_ZhCDnv"
      },
      "source": [
        "Suggested exercises:\n",
        "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
        "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
        "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
        "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}